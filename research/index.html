<!DOCTYPE html>
<html lang="en" data-dark="false">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!--
  put your analytics (e.g. Google Analytics) tracking code here
-->

  <!--
  put your search engine verification (e.g. Google Search Console) tag here
-->

  


























<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Research | Trustworthy Autonomous Systems Laboratory (TASL)</title>

<link rel="icon" href="/images/icon.png">

<meta name="title" content="Research">
<meta name="description" content="">

<meta property="og:title" content="Research">
<meta property="og:site_title" content="Trustworthy Autonomous Systems Laboratory (TASL)">
<meta property="og:description" content="">
<meta property="og:url" content="http://tasl.ucr.edu">
<meta property="og:image" content="/images/share.jpg">
<meta property="og:locale" content="en_US">

<meta property="twitter:title" content="Research">
<meta property="twitter:description" content="">
<meta property="twitter:url" content="http://tasl.ucr.edu">
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="/images/share.jpg">


  <meta property="og:type" content="website">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "WebSite",
    
    "name": "Research",
    "description": "",
    "headline": "Research",
    "publisher": {
      "@type": "Organization",
      "logo": { "@type": "ImageObject", "url": "/images/icon.png" }
    },
    "url": "http://tasl.ucr.edu"
  }
</script>

<link rel="alternate" type="application/rss+xml" href="http://tasl.ucr.edu/feed.xml">

  <!-- Google Fonts -->
<!-- automatically get url from fonts used in theme file -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?display=swap&&family=Barlow:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto+Mono:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600" rel="stylesheet">

<!-- Font Awesome icons (load asynchronously due to size) -->

<link href="https://use.fontawesome.com/releases/v6.5.0/css/all.css" rel="stylesheet" media="none" onload="this.removeAttribute('media'); this.onload = null;">
<noscript>
  <link href="https://use.fontawesome.com/releases/v6.5.0/css/all.css" rel="stylesheet">
</noscript>

  <!-- third party styles -->
<!-- https://stylishthemes.github.io/Syntax-Themes/pygments/ -->
<link href="https://cdn.jsdelivr.net/gh/StylishThemes/Syntax-Themes/pygments/css-github/pygments-tomorrow-night-eighties.css" rel="stylesheet">

<!-- include all sass in styles folder -->


  
    <link href="/_styles/-theme.css" rel="stylesheet">
  

  
    <link href="/_styles/alert.css" rel="stylesheet">
  

  
    <link href="/_styles/all.css" rel="stylesheet">
  

  
    <link href="/_styles/anchor.css" rel="stylesheet">
  

  
    <link href="/_styles/background.css" rel="stylesheet">
  

  
    <link href="/_styles/body.css" rel="stylesheet">
  

  
    <link href="/_styles/bold.css" rel="stylesheet">
  

  
    <link href="/_styles/button.css" rel="stylesheet">
  

  
    <link href="/_styles/card.css" rel="stylesheet">
  

  
    <link href="/_styles/card_facilities.css" rel="stylesheet">
  

  
    <link href="/_styles/card_research.css" rel="stylesheet">
  

  
    <link href="/_styles/checkbox.css" rel="stylesheet">
  

  
    <link href="/_styles/citation.css" rel="stylesheet">
  

  
    <link href="/_styles/code.css" rel="stylesheet">
  

  
    <link href="/_styles/cols.css" rel="stylesheet">
  

  
    <link href="/_styles/dark-toggle.css" rel="stylesheet">
  

  
    <link href="/_styles/feature.css" rel="stylesheet">
  

  
    <link href="/_styles/figure.css" rel="stylesheet">
  

  
    <link href="/_styles/float.css" rel="stylesheet">
  

  
    <link href="/_styles/font.css" rel="stylesheet">
  

  
    <link href="/_styles/footer.css" rel="stylesheet">
  

  
    <link href="/_styles/form.css" rel="stylesheet">
  

  
    <link href="/_styles/grid.css" rel="stylesheet">
  

  
    <link href="/_styles/header.css" rel="stylesheet">
  

  
    <link href="/_styles/heading.css" rel="stylesheet">
  

  
    <link href="/_styles/highlight.css" rel="stylesheet">
  

  
    <link href="/_styles/icon.css" rel="stylesheet">
  

  
    <link href="/_styles/image.css" rel="stylesheet">
  

  
    <link href="/_styles/link.css" rel="stylesheet">
  

  
    <link href="/_styles/list.css" rel="stylesheet">
  

  
    <link href="/_styles/main.css" rel="stylesheet">
  

  
    <link href="/_styles/paragraph.css" rel="stylesheet">
  

  
    <link href="/_styles/portrait-pi.css" rel="stylesheet">
  

  
    <link href="/_styles/portrait-students.css" rel="stylesheet">
  

  
    <link href="/_styles/portrait.css" rel="stylesheet">
  

  
    <link href="/_styles/post-excerpt.css" rel="stylesheet">
  

  
    <link href="/_styles/post-info.css" rel="stylesheet">
  

  
    <link href="/_styles/post-nav.css" rel="stylesheet">
  

  
    <link href="/_styles/quote.css" rel="stylesheet">
  

  
    <link href="/_styles/rule.css" rel="stylesheet">
  

  
    <link href="/_styles/search-box.css" rel="stylesheet">
  

  
    <link href="/_styles/search-info.css" rel="stylesheet">
  

  
    <link href="/_styles/section.css" rel="stylesheet">
  

  
    <link href="/_styles/table.css" rel="stylesheet">
  

  
    <link href="/_styles/tags.css" rel="stylesheet">
  

  
    <link href="/_styles/textbox.css" rel="stylesheet">
  

  
    <link href="/_styles/tooltip.css" rel="stylesheet">
  

  
    <link href="/_styles/util.css" rel="stylesheet">
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  


<!-- include all css in styles folder -->



  <!-- third party scripts -->
<script src="https://unpkg.com/@popperjs/core@2" defer></script>
<script src="https://unpkg.com/tippy.js@6" defer></script>
<script src="https://unpkg.com/mark.js@8" defer></script>

<!-- include all js in scripts folder -->


  <script src="/_scripts/anchors.js"></script>

  <script src="/_scripts/dark-mode.js"></script>

  <script src="/_scripts/fetch-tags.js"></script>

  <script src="/_scripts/search.js"></script>

  <script src="/_scripts/site-search.js"></script>

  <script src="/_scripts/table-wrap.js"></script>

  <script src="/_scripts/tooltip.js"></script>


  <link rel="icon" href="/favicon.ico">
</head>
  <body>
    







<header class="background" style="--image: url('/images/background.jpg')" data-dark="true">
  <a href="/" class="home">
    
      <span class="logo">
        
          <?xml version="1.0" encoding="UTF-8" standalone="no"?>

<svg width="100%" height="100%" viewbox="0 0 3301 2000" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xmlns:serif="http://www.serif.com/" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;">
    <g transform="matrix(1,0,0,1,-76.8786,-289.144)">
        <g transform="matrix(3.42537,0,0,3.42537,-1898.64,-1602.52)">
            <path d="M745.223,785.195L1071.83,696.5L1183.27,791.175L1238.46,791.524L1251.99,761.461L1147.63,675.896L1309.54,631.871L1336.03,572.618L576.982,786.187L745.223,785.195Z" style="fill:rgb(185,19,0);"></path>
        </g>
        <g transform="matrix(3.42537,0,0,3.42537,-1898.64,-1602.52)">
            <path d="M576.819,789.509L1236.3,796.43L1280.02,840.758L1147.26,838.281L1180.56,891.845L1333.29,923.807L1360.95,977.41L576.819,789.509ZM901.066,834.01L1067.84,868.623L1057.82,836.721L901.066,834.01Z" style="fill:rgb(247,129,0);"></path>
        </g>
        <g transform="matrix(3.42537,0,0,3.42537,-1943.75,-1648.28)">
            <path d="M780.176,853.887L589.9,806.626L1361.78,1149.4L1354.52,994.146L1311.69,983.726L1308.99,1079.22L780.176,853.887Z" style="fill:rgb(170,213,238);"></path>
        </g>
        <g transform="matrix(3.42537,0,0,3.42537,-1898.64,-1602.52)">
            <path d="M1357.97,566.431L1408.31,552.251L1310.84,792.649L1540.37,1017.45L1372.96,978.3L1346.66,926.736L1404.75,938.057L1259.72,794.157L1357.97,566.431Z" style="fill:rgb(255,184,28);"></path>
        </g>
    </g>
</svg>

        
      </span>
    
    
      <span class="title-text">
        
          <span class="title">Trustworthy Autonomous Systems Laboratory (TASL)</span>
        
        
      </span>
    
  </a>

  <input class="nav-toggle" type="checkbox" aria-label="show/hide nav">

  <nav>
    
    
      
        <a href="/news/" data-tooltip="">
          News
        </a>
      
    
      
        <a href="/research/" data-tooltip="">
          Research
        </a>
      
    
      
        <a href="/publications/" data-tooltip="">
          Publications
        </a>
      
    
      
        <a href="/facilities/" data-tooltip="">
          Facilities
        </a>
      
    
      
        <a href="/teaching/" data-tooltip="">
          Teaching
        </a>
      
    
      
        <a href="/people/" data-tooltip="">
          People
        </a>
      
    
      
        <a href="/join/" data-tooltip="">
          Join Us
        </a>
      
    
  </nav>
</header>

    <main>
      <!--
  modify main content of page:
  - add section breaks
  - attach section properties
  - filter out blank sections
-->






  
  
  

  <section class="background" data-size="page">
    <h1 id="featured-research">
<i class="icon fa-solid fa-robot"></i>Featured Research</h1>

<p>The ultimate goal of our research is to build <strong>trustworthy</strong>, <strong>interactive</strong>, and <strong>human-centered</strong> autonomous embodied agents that can perceive, understand, and reason about the physical world; safely interact and collaborate with humans; and efficiently coordinate with other intelligent agents so that they can benefit society in daily lives. To achieve this goal, we have been pursuing interdisciplinary research and unifying the techniques and tools from robotics, trustworthy AI/ML, deep reinforcement learning, control theory, optimization, and computer vision.</p>

<!-- 




 -->

<div class="card" data-style="" style="width: 100%; ">

  <div class="card-text">
    
      <a class="card-title">
        Safe, Efficient, and Robust Decision Making for Human-Robot Interactions
      </a>
    

    
  </div>

  <div class="card-img" style="aspect-ratio: 5.0; overflow: hidden; margin-bottom: 0px;">
    <a aria-label="Safe, Efficient, and Robust Decision Making for Human-Robot Interactions" class="card-image">
      <img src="/research_images/human_robot_interaction.png" alt="Safe, Efficient, and Robust Decision Making for Human-Robot Interactions" loading="lazy" style="width: 100%; height: auto; object-fit: contain;" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  </div>
  
  <div class="card-description" style="margin: 0px;">
    
      <p>
        Although autonomous navigation in simple, static environments has been well studied, it remains challenging for robots to navigate 
in highly dynamic, interactive scenarios (e.g., intersections, narrow corridors) where humans are involved. 
Robots must learn a safe and efficient behavior policy that can model the interactions, take into account the uncertainties among 
the interactions during decision making, coordinate with surrounding static and dynamic entities, and generalize to 
out-of-distribution (OOD) situations.
In our research, we have
<strong>
1) introduced a novel interaction-aware decision making framework for autonomous vehicles based on deep reinforcement learning (DRL), 
which integrates human internal state inference, domain knowledge, trajectory prediction, and counterfactual reasoning systematically; 
2) developed a novel guided meta RL paradigm to improve the generalizability of learned policies and an importance sampling based 
training mechansim for unbiased policy learning;
3) investigated DRL methods that leverage the learned pairwise and group-wise relations for social robot navigation around human crowds;
and 4) proposed the first DRL framework that integrates the prediction uncertainty of pedestrians obtained from adaptative conformal 
inference and explicitly guides the policy learning process in a principled manner for social navigation.
</strong>
These approaches achieve superior performance in the corresponding tasks and provide explainable, human-understandable intermediate 
representations to build trust with humans.<br><br>

<strong>Related Publications:</strong> <br>
1. <a href="https://arxiv.org/abs/2407.17460">SoNIC: Safe Social Navigation with Adaptive Conformal Inference and Constrained Reinforcement Learning</a>, submitted to IEEE Robotics and Automation Letters (RA-L), under review. <br>
2. <a href="https://arxiv.org/abs/2401.12275">Multi-Agent Dynamic Relational Reasoning for Social Robot Navigation</a>, submitted to IEEE Transactions on Robotics (T-RO), under review. <br>
3. <a href="https://arxiv.org/abs/2407.09475">Importance Sampling-Guided Meta-Training for Intelligent Agents in Highly Interactive Environments</a>, submitted to IEEE Robotics and Automation Letters (RA-L), under review. <br>
4. <a href="https://arxiv.org/abs/2311.16091">Interactive Autonomous Navigation with Internal State Inference and Interactivity Estimation</a>, IEEE Transactions on Robotics (T-RO), 2024. <br>
5. <a href="https://arxiv.org/abs/2307.10160">Robust Driving Policy Learning with Guided Meta Reinforcement Learning</a>, ITSC 2023. <br>
6. <a href="https://www.researchgate.net/publication/374831905_Game_Theory-Based_Simultaneous_Prediction_and_Planning_for_Autonomous_Vehicle_Navigation_in_Crowded_Environments">Game Theory-Based Simultaneous Prediction and Planning for Autonomous Vehicle Navigation in Crowded Environments</a>, ITSC 2023. <br>
7. <a href="https://arxiv.org/abs/2106.13052">Autonomous Driving Strategies at Intersections: Scenarios, State-of-the-Art, and Future Outlooks</a>, ITSC 2021. <br>
8. <a href="https://arxiv.org/abs/2011.04251">Reinforcement Learning for Autonomous Driving with Latent State Inference and Spatial-Temporal Relationships</a>, ICRA 2021. <br>
9. <a href="https://arxiv.org/abs/2108.00716">Orientation-Aware Planning for Parallel Task Execution of Omni-Directional Mobile Robot</a>, IROS 2021. <br>
10. <a href="https://ieeexplore.ieee.org/abstract/document/8216790">Safe and Feasible Motion Generation for Autonomous Driving via Constrained Policy Net</a>, IECON 2017. <br>

      </p>
    
  
    
      


  <div class="tags" data-link="http://tasl.ucr.edu/research/">
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20social-navigation%22" class="tag" data-tooltip='Show items with the tag "social-navigation"'>
        social-navigation
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20autonomous-driving%22" class="tag" data-tooltip='Show items with the tag "autonomous-driving"'>
        autonomous-driving
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20human-robot-interaction/collaboration%22" class="tag" data-tooltip='Show items with the tag "human-robot-interaction/collaboration"'>
        human-robot-interaction/collaboration
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20trajectory-prediction%22" class="tag" data-tooltip='Show items with the tag "trajectory-prediction"'>
        trajectory-prediction
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20deep-reinforcement-learning%22" class="tag" data-tooltip='Show items with the tag "deep-reinforcement-learning"'>
        deep-reinforcement-learning
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20uncertainty-quantification%22" class="tag" data-tooltip='Show items with the tag "uncertainty-quantification"'>
        uncertainty-quantification
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20meta-learning%22" class="tag" data-tooltip='Show items with the tag "meta-learning"'>
        meta-learning
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20safety%22" class="tag" data-tooltip='Show items with the tag "safety"'>
        safety
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20generalization/robustness%22" class="tag" data-tooltip='Show items with the tag "generalization/robustness"'>
        generalization/robustness
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20explainability/interpretability%22" class="tag" data-tooltip='Show items with the tag "explainability/interpretability"'>
        explainability/interpretability
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20graph-neural-networks%22" class="tag" data-tooltip='Show items with the tag "graph-neural-networks"'>
        graph-neural-networks
      </a>
    
  </div>


    
  </div>
</div>

<div class="card" data-style="" style="width: 100%; ">

  <div class="card-text">
    
      <a class="card-title">
        Multi-Modal Foundation Models for Embodied Intelligence
      </a>
    

    
  </div>

  <div class="card-img" style="aspect-ratio: 5.0; overflow: hidden; margin-bottom: 0px;">
    <a aria-label="Multi-Modal Foundation Models for Embodied Intelligence" class="card-image">
      <img src="/research_images/robotics_foundation_model.png" alt="Multi-Modal Foundation Models for Embodied Intelligence" loading="lazy" style="width: 100%; height: auto; object-fit: contain;" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  </div>
  
  <div class="card-description" style="margin: 0px;">
    
      <p>
        While larger language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable proficiency in comprehending natural language and translating human instructions into detailed plans for straightforward robotic tasks, they encounter significant difficulties when tackling long-horizon complex tasks. 
In particular, the challenges of sub-task identification and allocation become especially complicated in scenarios involving cooperative teams of heterogeneous robots. 
To address these challenges, we have <strong>proposed a novel multi-agent task planning framework designed to excel in long-horizon tasks, which integrates the reasoning capabilities of LLMs with traditional heuristic search planning, which achieves high success rates and efficiency while demonstrating robust generalization across various tasks.</strong> 
This research not only contributes to the advancement of task planning for heterogeneous robotic teams but also lays the groundwork for future explorations in multi-agent collaboration.
<br><br>

<strong>Related Publications:</strong> <br>
1. <a href="https://arxiv.org/abs/2409.20560">LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and Planning with LM-Driven PDDL Planner</a>, submitted to IEEE International Conference on Robotics and Automation (ICRA 2025), under review. <br>

      </p>
    
  
    
      


  <div class="tags" data-link="http://tasl.ucr.edu/research/">
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20foundation-models%22" class="tag" data-tooltip='Show items with the tag "foundation-models"'>
        foundation-models
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20vision-language%22" class="tag" data-tooltip='Show items with the tag "vision-language"'>
        vision-language
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20mobile-manipulation%22" class="tag" data-tooltip='Show items with the tag "mobile-manipulation"'>
        mobile-manipulation
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20object-navigation%22" class="tag" data-tooltip='Show items with the tag "object-navigation"'>
        object-navigation
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20human-robot-interaction/collaboration%22" class="tag" data-tooltip='Show items with the tag "human-robot-interaction/collaboration"'>
        human-robot-interaction/collaboration
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20multi-agent/robot-systems%22" class="tag" data-tooltip='Show items with the tag "multi-agent/robot-systems"'>
        multi-agent/robot-systems
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20cooperative-planning%22" class="tag" data-tooltip='Show items with the tag "cooperative-planning"'>
        cooperative-planning
      </a>
    
  </div>


    
  </div>
</div>

<div class="card" data-style="" style="width: 100%; ">

  <div class="card-text">
    
      <a class="card-title">
        Cooperative Perception, Prediction, and Planning for Multi-Agent Systems
      </a>
    

    
  </div>

  <div class="card-img" style="aspect-ratio: 5.0; overflow: hidden; margin-bottom: 0px;">
    <a aria-label="Cooperative Perception, Prediction, and Planning for Multi-Agent Systems" class="card-image">
      <img src="/research_images/cooperative_pred_plan.png" alt="Cooperative Perception, Prediction, and Planning for Multi-Agent Systems" loading="lazy" style="width: 100%; height: auto; object-fit: contain;" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  </div>
  
  <div class="card-description" style="margin: 0px;">
    
      <p>
        Mobile robots and autonomous vehicles rely heavily on onboard sensors for perceiving and understanding the surroundings, 
and thereby learning safe and efficient planning strategies. While deep learning-based perception methods demonstrate impressive abilities in various tasks, 
including 2D/3D object detection, occupancy prediction, segmentation, tracking, such dependency is vulnerable to situations with occlusions, impaired visibility, and long-range perception. 
With the advancement of multi-agent communication, connected and automated vehicles (CAVs) can overcome the inherent limitations of single autonomous vehicle and mobile robots can collaboratively navigate complex environments. 
Our research <strong>firstly demonstrates enhanced situational awareness by sharing information between CAVs on both perception and motion prediction modules. 
Our framework design is robust to tolerate realistic V2X bandwidth limitations and transmission delays. Through extensive experiments and ablation studies on both simulated 
and real-world V2V datasets, we demonstrate the effectiveness of our method in cooperative perception, tracking, and motion prediction.</strong> 
This work advances multi-agent cooperation in robotics and autonomous driving systems.
<br><br>

<strong>Related Publications:</strong> <br>
1. <a href="https://arxiv.org/abs/2403.17916">CMP: Cooperative Motion Prediction with Multi-Agent Communication</a>, submitted to IEEE Robotics and Automation Letters, under review. <br>

      </p>
    
  
    
      


  <div class="tags" data-link="http://tasl.ucr.edu/research/">
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20cooperative-perception%22" class="tag" data-tooltip='Show items with the tag "cooperative-perception"'>
        cooperative-perception
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20cooperative-prediction%22" class="tag" data-tooltip='Show items with the tag "cooperative-prediction"'>
        cooperative-prediction
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20cooperative-planning%22" class="tag" data-tooltip='Show items with the tag "cooperative-planning"'>
        cooperative-planning
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20multi-agent/robot-systems%22" class="tag" data-tooltip='Show items with the tag "multi-agent/robot-systems"'>
        multi-agent/robot-systems
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20trajectory-prediction%22" class="tag" data-tooltip='Show items with the tag "trajectory-prediction"'>
        trajectory-prediction
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20connected-and-automated-vehicles%22" class="tag" data-tooltip='Show items with the tag "connected-and-automated-vehicles"'>
        connected-and-automated-vehicles
      </a>
    
  </div>


    
  </div>
</div>

<div class="card" data-style="" style="width: 100%; ">

  <div class="card-text">
    
      <a class="card-title">
        Vision Language Reasoning for Scene Understanding in Autonomous Driving
      </a>
    

    
  </div>

  <div class="card-img" style="aspect-ratio: 5.0; overflow: hidden; margin-bottom: 0px;">
    <a aria-label="Vision Language Reasoning for Scene Understanding in Autonomous Driving" class="card-image">
      <img src="/research_images/vision_language_driving.png" alt="Vision Language Reasoning for Scene Understanding in Autonomous Driving" loading="lazy" style="width: 100%; height: auto; object-fit: contain;" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  </div>
  
  <div class="card-description" style="margin: 0px;">
    
      <p>
        We investigate foundation models and vision language models (VLMs) for robotics and autonomous systems to enhance their reasoning capability and reliability. 
For example, inferring the short-term and long-term intentions of traffic participants and understanding the contextual semantics of scenes are the keys to scene understanding and situational awareness of autonomous vehicles. 
Moreover, how to enable autonomous agents (e.g., self-driving cars) to explain their reasoning, prediction, and decision making processes to human users (e.g., drivers, passengers) in a human understandable form (e.g., natural language) to build humans’ trust remains largely underexplored. 
Therefore, <strong>we created the first multimodal dataset for a new risk object ranking and natural language explanation task in urban scenarios and a rich dataset for intention prediction in autonomous driving, establishing benchmarks for corresponding tasks. Meanwhile, our research introduced novel methods that achieve superior performance on these problems.</strong>
<br><br>

<strong>Related Publications:</strong> <br>
1. <a href="https://arxiv.org/abs/2309.06597">Rank2Tell: A Multimodal Dataset for Joint Driving Importance Ranking and Reasoning</a>, WACV 2024. <br>
2. <a href="https://arxiv.org/abs/2209.10767">DRAMA: Joint Risk Localization and Captioning in Driving</a>, WACV 2023. <br>
3. <a href="https://arxiv.org/abs/2203.02634">Important Object Identification with Semi-Supervised Learning for Autonomous Driving</a>, ICRA 2022. <br>
4. <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Girase_LOKI_Long_Term_and_Key_Intentions_for_Trajectory_Prediction_ICCV_2021_paper.html">LOKI: Long Term and Key Intentions for Trajectory Prediction</a>, ICCV 2021.

      </p>
    
  
    
      


  <div class="tags" data-link="http://tasl.ucr.edu/research/">
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20foundation-models%22" class="tag" data-tooltip='Show items with the tag "foundation-models"'>
        foundation-models
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20vision-language%22" class="tag" data-tooltip='Show items with the tag "vision-language"'>
        vision-language
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20autonomous-driving%22" class="tag" data-tooltip='Show items with the tag "autonomous-driving"'>
        autonomous-driving
      </a>
    
  </div>


    
  </div>
</div>

<div class="card" data-style="" style="width: 100%; ">

  <div class="card-text">
    
      <a class="card-title">
        Explainable and Generalizable Relational Reasoning and Multi-Agent Interaction Modeling
      </a>
    

    
  </div>

  <div class="card-img" style="aspect-ratio: 5.0; overflow: hidden; margin-bottom: 0px;">
    <a aria-label="Explainable and Generalizable Relational Reasoning and Multi-Agent Interaction Modeling" class="card-image">
      <img src="/research_images/Explainable%20Relational%20Reasoning.webp" alt="Explainable and Generalizable Relational Reasoning and Multi-Agent Interaction Modeling" loading="lazy" style="width: 100%; height: auto; object-fit: contain;" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  </div>
  
  <div class="card-description" style="margin: 0px;">
    
      <p>
        We investigate dynamic relational reasoning and interaction modeling under the context of the trajectory/motion prediction task, which aims to generate accurate, diverse future trajectory hypotheses or state sequences based on historical observations.
<strong>Our research introduced the first unified relational reasoning toolbox that systematically infers the underlying relations/interactions between entities at different scales (e.g., pairwise, group-wise) and different abstraction levels (e.g., multiplex) by learning dynamic latent interaction graphs and hypergraphs from observable states (e.g., positions) in an unsupervised manner.</strong>
The learned latent graphs are explainable and generalizable, significantly improving the performance of downstream tasks, including more accurate and generalizable prediction as well as safer and more efficient sequential decision making and control for mobile robots.
<strong>We also proposed a physics-guided relational learning approach for physical dynamics modeling, which accurately simulates and infers future evolution of physical systems.</strong><br><br>

<strong>Related Publications:</strong> <br>
1. <a href="https://arxiv.org/abs/2401.12275">Multi-Agent Dynamic Relational Reasoning for Social Robot Navigation</a>, submitted to IEEE Transactions on Robotics (T-RO), under review. <br>
2. <a href="https://arxiv.org/abs/2311.16091">Interactive Autonomous Navigation with Internal State Inference and Interactivity Estimation</a>, IEEE Transactions on Robotics (T-RO), 2024. <br>
3. <a href="https://arxiv.org/abs/2109.14128">Grouptron: Dynamic Multi-Scale Graph Convolutional Networks for Group-Aware Crowd Trajectory Forecasting</a>, ICRA 2022. <br>
4. <a href="http://arxiv.org/abs/2203.02634">Important Object Identification with Semi-Supervised Learning for Autonomous Driving</a>, ICRA 2022. <br>
5. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/a845fdc3f87751710218718adb634fe7-Abstract-Conference.html">Learning Physical Dynamics with Subequivariant Graph Neural Networks</a>, NeurIPS 2022. <br>
6. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/7e6361a5d73a8fab093dd8453e0b106f-Abstract-Conference.html">Interaction Modeling with Multiplex Attention</a>, NeurIPS 2022. <br>
7. <a href="https://ieeexplore.ieee.org/document/9491972">Spatio-Temporal Graph Dual-Attention Network for Multi-Agent Prediction and Tracking</a>, IEEE Transactions on Intelligent Transportation Systems, 2022. <br>
8. <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Li_RAIN_Reinforced_Hybrid_Attention_Inference_Network_for_Motion_Forecasting_ICCV_2021_paper.html">RAIN: Reinforced Hybrid Attention Inference Network for Motion Forecasting</a>, ICCV 2021. <br>
9. <a href="https://ieeexplore.ieee.org/document/9512468">Continual Multi-agent Interaction Behavior Prediction with Conditional Generative Memory</a>, IEEE Robotics and Automation Letters, 2021. <br>
10. <a href="https://arxiv.org/abs/2106.02930">Spectral Temporal Graph Neural Network for Trajectory Prediction</a>, ICRA 2021. <br>
11. <a href="https://proceedings.neurips.cc/paper/2020/hash/e4d8163c7a068b65a64c89bd745ec360-Abstract.html">EvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational Reasoning</a>, NeurIPS 2020. <br>

      </p>
    
  
    
      


  <div class="tags" data-link="http://tasl.ucr.edu/research/">
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20relational-reasoning%22" class="tag" data-tooltip='Show items with the tag "relational-reasoning"'>
        relational-reasoning
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20graph-neural-networks%22" class="tag" data-tooltip='Show items with the tag "graph-neural-networks"'>
        graph-neural-networks
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20multi-agent-interaction%22" class="tag" data-tooltip='Show items with the tag "multi-agent-interaction"'>
        multi-agent-interaction
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20trajectory-prediction%22" class="tag" data-tooltip='Show items with the tag "trajectory-prediction"'>
        trajectory-prediction
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20explainability/interpretability%22" class="tag" data-tooltip='Show items with the tag "explainability/interpretability"'>
        explainability/interpretability
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20generalization%22" class="tag" data-tooltip='Show items with the tag "generalization"'>
        generalization
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20autonomous-driving%22" class="tag" data-tooltip='Show items with the tag "autonomous-driving"'>
        autonomous-driving
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20social-navigation%22" class="tag" data-tooltip='Show items with the tag "social-navigation"'>
        social-navigation
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20human-robot-interaction/collaboration%22" class="tag" data-tooltip='Show items with the tag "human-robot-interaction/collaboration"'>
        human-robot-interaction/collaboration
      </a>
    
  </div>


    
  </div>
</div>

<div class="card" data-style="" style="width: 100%; ">

  <div class="card-text">
    
      <a class="card-title">
        Generalizable and Diverse Trajectory and Occupancy Prediction for Autonomous Driving
      </a>
    

    
  </div>

  <div class="card-img" style="aspect-ratio: 5.0; overflow: hidden; margin-bottom: 0px;">
    <a aria-label="Generalizable and Diverse Trajectory and Occupancy Prediction for Autonomous Driving" class="card-image">
      <img src="/research_images/trajectory_occupancy_prediction.png" alt="Generalizable and Diverse Trajectory and Occupancy Prediction for Autonomous Driving" loading="lazy" style="width: 100%; height: auto; object-fit: contain;" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  </div>
  
  <div class="card-description" style="margin: 0px;">
    
      <p>
        Trajectory and occupancy prediction is a critical research area in the field of autonomous driving. 
As autonomous driving technology advances rapidly, accurately predicting the trajectories and occupancy of dynamic objects 
such as vehicles and pedestrians has become essential for enhancing the safety and reliability of autonomous systems. 
Effective trajectory and occupancy prediction enables autonomous vehicles to anticipate potential hazards in their environment, 
thereby improving decision making processes and reducing the risk of accidents. This directly contributes to the development 
of more robust and safe autonomous driving technologies. In our research, we have
<strong>
1) developed effective solutions to model the diverse and uncertain behavior of various traffic participants (e.g., vehicles, pedestrians,
cyclists) and infer their future trajectories and occupancy of the scene in highly complex and interactive traffic scenarios; 
2) investigated how to effectively detect and handle out-of-distribution (OOD) situations by improving the generalizability of prediction 
frameworks, which achieves state-of-the-art performance in cross-dateset OOD evaluations;
3) introduced the first-of-its-kind cooperative motion prediction framework that advances the capabilities of connected
and automated vehicles (CAVs) in cooperative tracking and motion prediction, addressing the crucial need for safe and robust decision making in dynamic environments.
</strong>
<br><br>

<strong>Related Publications:</strong> <br>
1. <a href="https://arxiv.org/abs/2403.17916">CMP: Cooperative Motion Prediction with Multi-Agent Communication</a>, submitted to IEEE Robotics and Automation Letters (RA-L), under review. <br>
2. <a href="https://arxiv.org/abs/2407.09475">Adaptive Prediction Ensemble: Improving Out-of-Distribution Generalization of Motion Forecasting</a>, submitted to IEEE Robotics and Automation Letters (RA-L), under review. <br>
3. <a href="https://arxiv.org/abs/2407.21126">Self-Supervised Multi-Future Occupancy Forecasting for Autonomous Driving</a>, submitted to IEEE Robotics and Automation Letters (RA-L), under review. <br>
4. <a href="https://arxiv.org/abs/2309.13893">Scene Informer: Anchor-based Occlusion Inference and Trajectory Prediction in Partially Observable Environments</a>, ICRA 2024. <br>
5. <a href="https://arxiv.org/abs/2310.01723">Predicting Future Spatiotemporal Occupancy Grids with Semantics for Autonomous Driving</a>, IV 2024. <br>
6. <a href="https://arxiv.org/abs/2306.01075">Pedestrian Crossing Action Recognition and Trajectory Prediction with 3D Human Keypoints</a>, ICRA 2023. <br>
7. <a href="https://ieeexplore.ieee.org/document/10422696">Game Theory-Based Simultaneous Prediction and Planning for Autonomous Vehicle Navigation in Crowded Environments</a>, ITSC 2023. <br>
8. <a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/itr2.12345">A Cognition-Inspired Trajectory Prediction Method for Vehicles in Interactive Scenarios</a>, IET Intelligent Transport Systems, 2023. <br>
9. <a href="https://arxiv.org/abs/2209.13172">Dynamics-Aware Spatiotemporal Occupancy Prediction in Urban Environments</a>, IROS 2022. <br>
10. <a href="https://arxiv.org/abs/2102.09117">Spatio-Temporal Graph Dual-Attention Network for Multi-Agent Prediction and Tracking</a>, IEEE Transactions on Intelligent Transportation Systems, 2022. <br>
11. <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Li_RAIN_Reinforced_Hybrid_Attention_Inference_Network_for_Motion_Forecasting_ICCV_2021_paper.html">RAIN: Reinforced Hybrid Attention Inference Network for Motion Forecasting</a>, ICCV 2021. <br>
12. <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Choi_Shared_Cross-Modal_Trajectory_Prediction_for_Autonomous_Driving_CVPR_2021_paper.html">Shared Cross-Modal Trajectory Prediction for Autonomous Driving</a>, CVPR 2021 (Oral). <br>
13. <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Girase_LOKI_Long_Term_and_Key_Intentions_for_Trajectory_Prediction_ICCV_2021_paper.html">LOKI: Long Term and Key Intentions for Trajectory Prediction</a>, ICCV 2021. <br>
14. <a href="https://ieeexplore.ieee.org/document/9512468">Continual Multi-agent Interaction Behavior Prediction with Conditional Generative Memory</a>, IEEE Robotics and Automation Letters (RA-L), 2021. <br>
15. <a href="https://ieeexplore.ieee.org/document/9564510">Multi-agent Driving Behavior Prediction across Different Scenarios with Self-supervised Domain Knowledge</a>, ITSC 2021. <br>
16. <a href="https://arxiv.org/abs/2003.13924">EvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational Reasoning</a>, NeurIPS 2020. <br>
17. <a href="https://arxiv.org/abs/1908.09031">Generic Tracking and Probabilistic Prediction Framework and Its Application in Autonomous Driving</a>, IEEE Transactions on Intelligent Transportation Systems, 2020. <br>
18. <a href="https://arxiv.org/abs/1904.02390">Interaction-aware Multi-agent Tracking and Probabilistic Behavior Prediction via Adversarial Learning</a>, ICRA 2019. <br>
19. <a href="https://arxiv.org/abs/1905.01631">Conditional Generative Neural System for Probabilistic Trajectory Prediction</a>, IROS 2019. <br>
20. <a href="https://arxiv.org/abs/1905.00587">Coordination and Trajectory Prediction for Vehicle Interactions via Bayesian Generative Modeling</a>, IV 2019. <br>
21. <a href="https://ieeexplore.ieee.org/document/8813783">Wasserstein Generative Learning with Kinematic Constraints for Probabilistic Interactive Driving Behavior Prediction</a>, IV 2019.

      </p>
    
  
    
      


  <div class="tags" data-link="http://tasl.ucr.edu/research/">
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20trajectory-prediction%22" class="tag" data-tooltip='Show items with the tag "trajectory-prediction"'>
        trajectory-prediction
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20occupancy-prediction%22" class="tag" data-tooltip='Show items with the tag "occupancy-prediction"'>
        occupancy-prediction
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20occlusion-inference%22" class="tag" data-tooltip='Show items with the tag "occlusion-inference"'>
        occlusion-inference
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20autonomous-driving%22" class="tag" data-tooltip='Show items with the tag "autonomous-driving"'>
        autonomous-driving
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20generative-models%22" class="tag" data-tooltip='Show items with the tag "generative-models"'>
        generative-models
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20continual/lifelong-learning%22" class="tag" data-tooltip='Show items with the tag "continual/lifelong-learning"'>
        continual/lifelong-learning
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20generalization/robustness%22" class="tag" data-tooltip='Show items with the tag "generalization/robustness"'>
        generalization/robustness
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20spatio-temporal-reasoning%22" class="tag" data-tooltip='Show items with the tag "spatio-temporal-reasoning"'>
        spatio-temporal-reasoning
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20game-theory%22" class="tag" data-tooltip='Show items with the tag "game-theory"'>
        game-theory
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20self-supervised-learning%22" class="tag" data-tooltip='Show items with the tag "self-supervised-learning"'>
        self-supervised-learning
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20relational-reasoning%22" class="tag" data-tooltip='Show items with the tag "relational-reasoning"'>
        relational-reasoning
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20graph-neural-networks%22" class="tag" data-tooltip='Show items with the tag "graph-neural-networks"'>
        graph-neural-networks
      </a>
    
  </div>


    
  </div>
</div>

<div class="card" data-style="" style="width: 100%; ">

  <div class="card-text">
    
      <a class="card-title">
        Human Intention and Motion Prediction for Human-Robot Interactions
      </a>
    

    
  </div>

  <div class="card-img" style="aspect-ratio: 5.0; overflow: hidden; margin-bottom: 0px;">
    <a aria-label="Human Intention and Motion Prediction for Human-Robot Interactions" class="card-image">
      <img src="/research_images/human_motion.png" alt="Human Intention and Motion Prediction for Human-Robot Interactions" loading="lazy" style="width: 100%; height: auto; object-fit: contain;" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  </div>
  
  <div class="card-description" style="margin: 0px;">
    
      <p>
        Human intention and motion prediction is a vital research area that focuses on improving the safety and efficiency of interactions 
between humans and robots. As robots are increasingly integrated into environments shared with humans, such as homes, workplaces, 
and healthcare settings, it becomes crucial to predict human intentions and movements accurately. Understanding human intentions 
allows robots to anticipate and respond to human actions in a way that is both intuitive and safe, thereby enhancing the quality of 
human-robot interactions. This contributes to the development of more intelligent and adaptive robotic systems that can seamlessly 
collaborate with humans in various real-world scenarios. In our research, we have
<strong>
1) developed multi-modal prediction methods for predicting human intentions and generating future motions (e.g., trajectories, 
human skeletons), which leverage fine-grained semantic and human appearance information.
2) proposed a systematic framework to identify generalizable dynamic relations (pairwise, group-wise) among human crowds.
3) introduced effective deep generative models to generate diverse, realistic human motions for human behavior simulation, 
which enhances the performance of downstream tasks.
<strong>
<br><br></strong></strong>

<strong>Related Publications:</strong> <br>
1. <a href="https://arxiv.org/abs/2407.17460">SoNIC: Safe Social Navigation with Adaptive Conformal Inference and Constrained Reinforcement Learning</a>, submitted to IEEE Robotics and Automation Letters (RA-L), under review. <br>
2. <a href="https://arxiv.org/abs/2401.12275">Multi-Agent Dynamic Relational Reasoning for Social Robot Navigation</a>, submitted to IEEE Transactions on Robotics (T-RO), under review. <br>
3. <a href="https://arxiv.org/abs/2403.06041">MATRIX: Multi-Agent Trajectory Generation with Diverse Contexts</a>, ICRA 2024. <br>
4. <a href="https://arxiv.org/abs/2306.01075">Pedestrian Crossing Action Recognition and Trajectory Prediction with 3D Human Keypoints</a>, ICRA 2023. <br>
5. <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Ma_Multi-Objective_Diverse_Human_Motion_Prediction_With_Knowledge_Distillation_CVPR_2022_paper.html">Multi-Objective Diverse Human Motion Prediction with Knowledge Distillation</a>, CVPR 2022 (Oral). <br>
6. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/7e6361a5d73a8fab093dd8453e0b106f-Abstract-Conference.html">Interaction Modeling with Multiplex Attention</a>, NeurIPS 2022. <br>
7. <a href="https://arxiv.org/abs/2109.14128">Grouptron: Dynamic Multi-Scale Graph Convolutional Networks for Group-Aware Crowd Trajectory Forecasting</a>, ICRA 2022. <br>
8. <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Li_RAIN_Reinforced_Hybrid_Attention_Inference_Network_for_Motion_Forecasting_ICCV_2021_paper.html">RAIN: Reinforced Hybrid Attention Inference Network for Motion Forecasting</a>, ICCV 2021. <br>
9. <a href="https://arxiv.org/abs/2106.02930">Spectral Temporal Graph Neural Network for Trajectory Prediction</a>, ICRA 2021. <br>
10. <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Girase_LOKI_Long_Term_and_Key_Intentions_for_Trajectory_Prediction_ICCV_2021_paper.html">LOKI: Long Term and Key Intentions for Trajectory Prediction</a>, ICCV 2021. <br>
11. <a href="https://arxiv.org/abs/2003.13924">EvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational Reasoning</a>, NeurIPS 2020. <br>
12. <a href="https://arxiv.org/abs/1905.01631">Conditional Generative Neural System for Probabilistic Trajectory Prediction</a>, IROS 2019. <br>

      </p>
    
  
    
      


  <div class="tags" data-link="http://tasl.ucr.edu/research/">
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20trajectory-prediction%22" class="tag" data-tooltip='Show items with the tag "trajectory-prediction"'>
        trajectory-prediction
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20human-inention/motion-prediction%22" class="tag" data-tooltip='Show items with the tag "human-inention/motion-prediction"'>
        human-inention/motion-prediction
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20human-robot-interaction/collaboration%22" class="tag" data-tooltip='Show items with the tag "human-robot-interaction/collaboration"'>
        human-robot-interaction/collaboration
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20social-navigation%22" class="tag" data-tooltip='Show items with the tag "social-navigation"'>
        social-navigation
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20generative-models%22" class="tag" data-tooltip='Show items with the tag "generative-models"'>
        generative-models
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20relational-reasoning%22" class="tag" data-tooltip='Show items with the tag "relational-reasoning"'>
        relational-reasoning
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20graph-neural-networks%22" class="tag" data-tooltip='Show items with the tag "graph-neural-networks"'>
        graph-neural-networks
      </a>
    
  </div>


    
  </div>
</div>

<div class="card" data-style="" style="width: 100%; ">

  <div class="card-text">
    
      <a class="card-title">
        Improving Generalizability by Learning Context Relations
      </a>
    

    
  </div>

  <div class="card-img" style="aspect-ratio: 5.0; overflow: hidden; margin-bottom: 0px;">
    <a aria-label="Improving Generalizability by Learning Context Relations" class="card-image">
      <img src="/research_images/context_relation.png" alt="Improving Generalizability by Learning Context Relations" loading="lazy" style="width: 100%; height: auto; object-fit: contain;" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  </div>
  
  <div class="card-description" style="margin: 0px;">
    
      <p>
        How to generalize the prediction to different scenarios is largely underexplored. 
In contrast to recent works that use the Cartesian coordinate system and global context images directly as input, <strong>we propose to leverage human prior knowledge including the comprehension of pairwise relations between agents and pairwise context information extracted by self-supervised learning approaches to attain an effective Frenet-based representation.</strong> 
We demonstrate that our approach achieves superior performance in terms of <strong>overall performance, zero-shot, and few-shot transferability across different traffic scenarios with diverse layouts.</strong> <br><br>

<strong>Related Publications:</strong> <br>
1. <a href="https://ieeexplore.ieee.org/document/9564510">Multi-Agent Driving Behavior Prediction across Different Scenarios with Self-Supervised Domain Knowledge</a>, ITSC 2021.

      </p>
    
  
    
      


  <div class="tags" data-link="http://tasl.ucr.edu/research/">
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20trajectory-prediction%22" class="tag" data-tooltip='Show items with the tag "trajectory-prediction"'>
        trajectory-prediction
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20generalization/robustness%22" class="tag" data-tooltip='Show items with the tag "generalization/robustness"'>
        generalization/robustness
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20self-supervised-learning%22" class="tag" data-tooltip='Show items with the tag "self-supervised-learning"'>
        self-supervised-learning
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20domain-knowledge%22" class="tag" data-tooltip='Show items with the tag "domain-knowledge"'>
        domain-knowledge
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20relational-reasoning%22" class="tag" data-tooltip='Show items with the tag "relational-reasoning"'>
        relational-reasoning
      </a>
    
  </div>


    
  </div>
</div>

<div class="card" data-style="" style="width: 100%; ">

  <div class="card-text">
    
      <a class="card-title">
        Continual/Lifelong Learning from Incremental Data
      </a>
    

    
  </div>

  <div class="card-img" style="aspect-ratio: 5.0; overflow: hidden; margin-bottom: 0px;">
    <a aria-label="Continual/Lifelong Learning from Incremental Data" class="card-image">
      <img src="/research_images/continual_learning.png" alt="Continual/Lifelong Learning from Incremental Data" loading="lazy" style="width: 100%; height: auto; object-fit: contain;" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  </div>
  
  <div class="card-description" style="margin: 0px;">
    
      <p>
        The current mainstream research focuses on how to achieve accurate prediction on one large dataset. 
However, whether the multi-agent trajectory prediction model can be trained with a sequence of datasets, i.e., continual learning settings, remains a question. 
Can the current prediction methods avoid catastrophic forgetting? Can we utilize the continual learning strategy in the multi-agent trajectory prediction application? 
Motivated by the generative replay methods in continual learning literature, <strong>we propose a multi-agent interaction behavior prediction framework with a graph neural network-based conditional generative memory system to mitigate catastrophic forgetting. 
To the best of our knowledge, this work is the first attempt to study the continual learning problem in multi-agent interaction behavior prediction problems.</strong> 
We empirically show that several approaches in literature indeed suffer from catastrophic forgetting, and our approach succeeds in maintaining a low prediction error when datasets come sequentially. <br><br>

<strong>Related Publications:</strong> <br>
1. <a href="https://ieeexplore.ieee.org/document/9512468">Continual Multi-Agent Interaction Behavior Prediction With Conditional Generative Memory</a>, IEEE Robotics and Automation Letters, 2021.

      </p>
    
  
    
      


  <div class="tags" data-link="http://tasl.ucr.edu/research/">
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20continual-learning%22" class="tag" data-tooltip='Show items with the tag "continual-learning"'>
        continual-learning
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20trajectory-prediction%22" class="tag" data-tooltip='Show items with the tag "trajectory-prediction"'>
        trajectory-prediction
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20relational-reasoning%22" class="tag" data-tooltip='Show items with the tag "relational-reasoning"'>
        relational-reasoning
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20generative-models%22" class="tag" data-tooltip='Show items with the tag "generative-models"'>
        generative-models
      </a>
    
  </div>


    
  </div>
</div>

<div class="card" data-style="" style="width: 100%; ">

  <div class="card-text">
    
      <a class="card-title">
        Multi-Object State Estimation with Learning-Based Models
      </a>
    

    
  </div>

  <div class="card-img" style="aspect-ratio: 5.0; overflow: hidden; margin-bottom: 0px;">
    <a aria-label="Multi-Object State Estimation with Learning-Based Models" class="card-image">
      <img src="/research_images/tracking.png" alt="Multi-Object State Estimation with Learning-Based Models" loading="lazy" style="width: 100%; height: auto; object-fit: contain;" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  </div>
  
  <div class="card-description" style="margin: 0px;">
    
      <p>
        We proposed a constrained mixture sequential Monte Carlo method that mitigates mode collapse in sequential Monte Carlo methods for tracking multiple targets and significantly improves tracking accuracy. 
Since prediction is a step in state estimation, we also <strong>proposed that the prior update in the state estimation framework can be implemented with any learning-based interaction-aware prediction model.</strong> 
The results in complex traffic scenarios show that using the prediction model outperforms purely physical models by a large margin due to the capability of relational reasoning. 
In particular, our method performs significantly better when handling missing or noisy sensor measurements.<br><br>

<strong>Related Publications:</strong> <br>
1. <a href="https://ieeexplore.ieee.org/document/9491972">Spatio-Temporal Graph Dual-Attention Network for Multi-Agent Prediction and Tracking</a>, IEEE Transactions on Intelligent Transportation Systems, 2022. <br>
2. <a href="https://arxiv.org/abs/1908.09031">Generic Tracking and Probabilistic Prediction Framework and Its Application in Autonomous Driving</a>, IEEE Transactions on Intelligent Transportation Systems, 2021. <br>
3. <a href="https://arxiv.org/abs/1904.02390">Interaction-aware Multi-agent Tracking and Probabilistic Behavior Prediction via Adversarial Learning</a>, ICRA 2019. <br>
4. <a href="https://arxiv.org/abs/1809.10237">Generic Vehicle Tracking Framework Capable of Handling Occlusions Based on Modified Mixture Particle Filter</a>, IV 2018.

      </p>
    
  
    
      


  <div class="tags" data-link="http://tasl.ucr.edu/research/">
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20multi-object-tracking%22" class="tag" data-tooltip='Show items with the tag "multi-object-tracking"'>
        multi-object-tracking
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20sequential-monte-carlo%22" class="tag" data-tooltip='Show items with the tag "sequential-monte-carlo"'>
        sequential-monte-carlo
      </a>
    
      <a href="http://tasl.ucr.edu/research/?search=%22tag:%20occlusion-handling%22" class="tag" data-tooltip='Show items with the tag "occlusion-handling"'>
        occlusion-handling
      </a>
    
  </div>


    
  </div>
</div>
  </section>


    </main>
    


<footer class="background" style="--image: url('/images/background.jpg')" data-dark="true" data-size="wide">
  <!--
    <div>
      Extra details like contact info or address
    </div>
  -->

  <div>
    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://github.com/tasl-lab" data-tooltip="GitHub" data-style="bare" aria-label="GitHub">
      <i class="icon fa-brands fa-github"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://youtube.com/@TASLaboratory" data-tooltip="YouTube" data-style="bare" aria-label="YouTube">
      <i class="icon fa-brands fa-youtube"></i>
      
    </a>
  </div>


    
  </div>

  <div>
    © 2024
    Trustworthy Autonomous Systems Laboratory (TASL)
     
  </div>

  <input type="checkbox" class="dark-toggle" data-tooltip="Dark mode" aria-label="toggle dark mode" oninput="onDarkToggleChange(event)">
</footer>

  </body>
</html>
